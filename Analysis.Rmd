---
title: "Usenet Analysis"
author: "Richard G. Gardiner"
date: "12/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(spelling)
spell_check_files("Analysis.Rmd")
```

This example comes from the final chapter of the Text Mining with R book.  Here we will use the common Usenet bulletin board dataset to run a "start to finish" analysis.  This dataset has 20,000 messages sent ot 20 Usenet boards in 1993.  The data is available at [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/) (use the `20news-bydate.tar.gz`)

## loading and pre-processing

We will start by reading all messages from the `20news-bydate.tar.gz` folder which are organized into subfolders with one file for each message.  We can do this using a combination of read_lines, map(), and unnest().

```{r, reading_data}
library(dplyr)
library(tidyr)
library(purrr)
library(readr)

training_folder <- "20news-bydate/20news-bydate-train/"

# define a function to read all files from a folder into a data frame
read_folder <- function(infolder) {
  data_frame(file = dir(infolder, full.names = TRUE)) %>%
    mutate(text = map(file, read_lines)) %>%
    transmute(id = basename(file), text) %>%
    unnest(text)
}

# use unnest() and map() to apply read_folder to each subfolder
raw_text <- data_frame(folder = dir(training_folder, full.names = TRUE)) %>%
  unnest(map(folder, read_folder)) %>%
  transmute(newsgroup = basename(folder), id, text)

raw_text
```

Note that hte `newsgroup` column, which describes which of hte 20 newsgroups each message comes from, and `id` column which identifies a unique message within that newsgroup.  What newsgroups are incldued, and how many messages were posted in each?

```{r}
library(ggplot2)

raw_text %>%
  group_by(newsgroup) %>%
  summarise(messages = n_distinct(id)) %>%
  ggplot(aes(x = newsgroup, y = messages)) +
  geom_col() +
  coord_flip()
```

We can see that the names are hierarchically grouped, starting with  main topic such as `"talk", "sci", or "rec".


### Pre-Processing Text


Unlike the other texts, this dataset has some strucutre and extra text that we don't want to analyze and thus needs to be removed.  For example, each message has a header, contaiing fields such as "from:" or "in_reply_to:" that describe the message.  Some also have automated email signatures, which occur after a line like `--`.  We can generally do this pre-processing suing the dplyr package, using a combination of `cumsum()` and `str_detect()` from stringr.

```{r}
library(stringr)

# must occur after the first occurence of an empty line,
# and before the first occurence of a line starting with --
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(text, "^--")) == 0) %>%
  ungroup()

cleaned_text
```

Many lines also have nested text representing quotes from other users, typically starting with a line like "so-and-so writes..." These can be removed with a few regular expression.  We alsochose to manually remove two messages, 9704 and 9985 which contain a large amount of non-text content.

```{r}
cleaned_text <- cleaned_text %>%
  filter(str_detect(text, "^[^.]+[A-Za-z\\d]") | text == "",
         !str_detect(text, "writes(:|\\.\\.\\.)$"),
         !str_detect(text, "^In article <"),
         !id %in% c(9704, 9985))
```


Now we can use the `unnest_tokens()` to split the dataset into tokens, while rmoving stop_words.

```{r}
library(tidytext)

unnest_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         word %in% stop_words$word)
```

Almost all datasets will require some cleaning and processing.  This happens through a lot of trial and error.  Though, as we can see, most of this can be done using the tidy tools like dplyr and tidyr.





